# ===================================================================
# MIND Project - Multi-Domain Pretraining Configuration
# ===================================================================
# This config enables training on multiple molecular dataset types simultaneously
# (proteins, small molecules, metabolites, RNA, DNA) with cross-modal batching.
#
# Key Features:
# - Atom-aware rotational sampling for balanced training
# - Cross-modal batches (proteins + small molecules in same batch)
# - Efficient memory usage (LRU cache with chunked datasets)
# - Graceful fallback (works even if some datasets are missing)
#
# Usage:
#   python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
# ===================================================================

# -------------------------
# Genel ve Çıktı Ayarları
# -------------------------
seed: 42
out_path: "outputs/multidomain_pretrain"
wandb_project_name: "MIND-Project-Pretraining"
wandb_run_name: "multidomain-run-1"

# -------------------------
# Multi-Domain Configuration
# -------------------------
multi_domain:
  enabled: true  # ✅ Set to true to enable multi-domain training
  
  # Dataset Configurations
  # - enabled: Whether to include this dataset type
  # - chunk_dir: Parent directory containing chunked .pt files
  # - atom_ratio: Target percentage of atoms from this dataset type (will be normalized)
  datasets:
    pdb:
      enabled: true
      chunk_dir: "/home/yusuf/data/proteins/processed_graphs_40k"  # Pattern: {this}_chunk_0, {this}_chunk_1, etc.
      atom_ratio: 70  # 70% of atoms from proteins
    
    qm9:
      enabled: false  # ⏳ Not yet available
      chunk_dir: "../data/small_molecules/qm9/processed_chunks"
      atom_ratio: 15  # 15% of atoms from QM9
    
    metabolite:
      enabled: false  # ⏳ Not yet available
      chunk_dir: "../data/metabolites/processed_chunks"
      atom_ratio: 10  # 10% of atoms from metabolites
    
    rna:
      enabled: false  # Future expansion
      chunk_dir: "../data/rna/processed_chunks"
      atom_ratio: 5   # 5% of atoms from RNA
    
    dna:
      enabled: false  # Future expansion
      chunk_dir: "../data/dna/processed_chunks"
      atom_ratio: 0
  
  # Rotational Sampling Parameters
  target_atoms_per_rotation: 50000  # Target total atoms per rotation (adjust based on GPU RAM)
  max_cache_chunks: 10              # Maximum chunks in LRU cache (adjust based on system RAM)
  fallback_to_single_domain: true   # If metadata missing, fall back to ChunkAwareSampler

# -------------------------
# Single-Domain Fallback Settings
# (Used when multi_domain.enabled = false OR only one dataset type is enabled)
# -------------------------
dataset: PDB
universal_cache_path: "data_loading/cache/universal_pdb_all.pkl"
dataset_download_dir: "/home/yusuf/data/proteins/processed_graphs_40k"

# -------------------------
# Data Loading Settings
# -------------------------
num_workers: 8
max_samples: null  # null = use all available samples
use_universal_cache: true

# -------------------------
# Eğitim Parametreleri
# -------------------------
max_epochs: 30
batch_size: 4  
lr: 0.0005
early_stopping_patience: 15
gradient_clip_val: 0.5
optimiser_weight_decay: 1.0e-10
monitor_loss_name: "train_total_loss"
use_bfloat16: false
accelerator: auto
devices: auto
precision: 16
num_sanity_val_steps: 0
fast_dev_run: false

# -------------------------
# Model Mimarisi (ESA)
# -------------------------
apply_attention_on: node
graph_dim: 256
edge_dim: 64
hidden_dims: [256, 256, 256, 256, 256, 256, 256, 256]
num_heads: [16, 16, 16, 16, 16, 16, 16, 16]
layer_types: ["M", "S", "M", "S", "M", "S", "P", "P"]
xformers_or_torch_attn: torch
norm_type: LN
use_3d_coordinates: true
set_max_items: 0
triu_attn_mask: false

# -------------------------
# MLP Ayarları
# -------------------------
use_mlps: true
mlp_hidden_size: 512
num_mlp_layers: 3
mlp_type: standard
mlp_dropout: 0.0
use_mlp_ln: false
pre_or_post: pre

# -------------------------
# Dropout Ayarları
# -------------------------
sab_dropout: 0.0
mab_dropout: 0.0
pma_dropout: 0.0
attn_residual_dropout: 0.0
pma_residual_dropout: 0.0

# -------------------------
# 3D Geometrik Parametreler
# -------------------------
molecule_max_atoms: null
gaussian_kernels: 128
cutoff_distance: 5.0
max_neighbors: 64
max_distance: 20.0
distance_bins: 5

# -------------------------
# Ön-Eğitim Görevleri
# -------------------------
pretraining_tasks: ["long_range_distance", "short_range_distance", "mlm"]
task_weights:
  long_range_distance: 1.5
  short_range_distance: 1.0
  mlm: 0.3
mlm_mask_ratio: 0.15
temperature: 0.2

# -------------------------
# Yeni Kod Yapısı İçin Gerekli Diğer Parametreler
# -------------------------
max_block_types: 150
max_position_types: 15
max_entity_types: 6
block_embedding_dim: 32
position_embedding_dim: 16
entity_embedding_dim: 8
use_hierarchical_features: false
max_atomic_num: 118
rwse_dim: 24
lap_dim: 4
mlp_inter_dim: 128
pma_num_outputs: 32
max_node_items: 2000
max_edge_items: 4000
output_dim: 1
posenc: null
use_memory_efficient_attention: true
attention_dropout: 0.1
preserve_universal_blocks: true

# ===================================================================
# Quick Start Guide
# ===================================================================
#
# 1. PROTEIN-ONLY MODE (Default):
#    - Set multi_domain.enabled = false
#    - Run: python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
#
# 2. MULTI-DOMAIN MODE (When you have QM9 and Metabolite data):
#    a. Generate metadata for all chunks:
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/proteins/processed_graphs_40k
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/small_molecules/qm9/processed_chunks
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/metabolites/processed_chunks
#
#    b. Update this config:
#       - Set multi_domain.enabled = true
#       - Set multi_domain.datasets.qm9.enabled = true
#       - Set multi_domain.datasets.metabolite.enabled = true
#       - Adjust atom_ratio values if needed (will be auto-normalized)
#
#    c. Run training:
#       python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
#
# 3. TUNING PARAMETERS:
#    - target_atoms_per_rotation: Increase if you have more GPU RAM, decrease if OOM
#    - max_cache_chunks: Increase if you have more system RAM
#    - atom_ratio: Adjust to control dataset contribution (doesn't need to sum to 100)
#
# ===================================================================

