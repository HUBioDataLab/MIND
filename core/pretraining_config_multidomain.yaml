# ===================================================================
# MIND Project - Multi-Domain Pretraining Configuration
# ===================================================================
# This config enables training on multiple molecular dataset types simultaneously
# (proteins, small molecules, metabolites, RNA, DNA) with cross-modal batching.
#
# Key Features:
# - Atom-aware rotational sampling for balanced training
# - Cross-modal batches (proteins + small molecules in same batch)
# - Efficient memory usage (LRU cache with chunked datasets)
# - Graceful fallback (works even if some datasets are missing)
#
# Usage:
#   python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
# ===================================================================

# -------------------------
# Genel ve Çıktı Ayarları
# -------------------------
seed: 42
out_path: "outputs/multidomain_pretrain"
wandb_project_name: "MIND-Project-Pretraining"
wandb_run_name: "multidomain-run-1"

# -------------------------
# Multi-Domain Configuration
# -------------------------
multi_domain:
  enabled: true  # ✅ Set to true to enable multi-domain training
  
  # Dataset Configurations
  # - enabled: Whether to include this dataset type
  # - chunk_dir: Parent directory containing chunked .pt files
  # - atom_ratio: Target percentage of atoms from this dataset type (will be normalized)
  datasets:
    pdb:
      enabled: true
      chunk_dir: "/home/yusuf/data/proteins/processed_14atom"  # 14-atom uniform representation chunks
      atom_ratio: 70  
      chunk_range: [0, 9]  # Use all 10 chunks
    
    qm9:
      enabled: true  
      chunk_dir: "/home/arin/tryout_mind/mind_try_merge/MIND/data_loading/qm9_chunks"
      atom_ratio: 30  
      chunk_range: null
    
    metabolite:
      enabled: false  # ⏳ Not yet available
      chunk_dir: "../data/metabolites/processed_chunks"
      atom_ratio: 10  # 10% of atoms from metabolites
    
    rna:
      enabled: false  # Future expansion
      chunk_dir: "../data/rna/processed_chunks"
      atom_ratio: 5   # 5% of atoms from RNA
    
    dna:
      enabled: false  # Future expansion
      chunk_dir: "../data/dna/processed_chunks"
      atom_ratio: 0
  
  # Rotational Sampling Parameters
  target_atoms_per_rotation: 50000  # Target total atoms per rotation (adjust based on GPU RAM)
  max_cache_chunks: 10              # Maximum chunks in LRU cache (adjust based on system RAM)
  fallback_to_single_domain: true   # If metadata missing, fall back to ChunkAwareSampler

# -------------------------
# Single-Domain Fallback Settings
# (Used when multi_domain.enabled = false OR only one dataset type is enabled)
# -------------------------
dataset: PDB
universal_cache_path: "data_loading/cache/universal_pdb_all.pkl"
dataset_download_dir: "/home/yusuf/data/proteins/processed_14atom"  # 14-atom uniform chunks

# -------------------------
# Data Loading Settings
# -------------------------
num_workers: 0  # Required for dynamic batching with lazy datasets (avoids hanging)
max_samples: null  # null = use all available samples
use_universal_cache: true

# -------------------------
# Eğitim Parametreleri
# -------------------------
max_epochs: 30
batch_size: 32  # Used for validation/test (training uses dynamic batching)
use_dynamic_batching: true  # ⚡ Essential for mixed datasets - enables atom-aware batching
use_improved_sampler: true  # ✅ Use improved sampler (requires metadata, much faster)
max_atoms_per_batch: 5000  # Target atoms per batch (smart batch ordering eliminates cache thrashing)
log_batch_statistics: true  # Print batch composition statistics during training
batch_stats_log_frequency: 10  # Log every N batches

# Per-Type Loss Tracking (for multi-domain analysis)
compute_per_type_losses: true   # ✅ Track losses per dataset type (protein, qm9, etc.)
log_per_type_frequency: 10      # Log per-type losses every N batches (reduce overhead)  

lr: 0.0005
early_stopping_patience: 15
gradient_clip_val: 0.5
optimiser_weight_decay: 1.0e-10
monitor_loss_name: "train_total_loss"
use_bfloat16: true  # Required for flash_varlen
accelerator: gpu
devices: 1  # Use single GPU (will be GPU 6 via CUDA_VISIBLE_DEVICES)
precision: bfloat16  # Required for flash_varlen
num_sanity_val_steps: 0
val_check_interval: 1.0  # Run validation at end of each epoch
fast_dev_run: false

# -------------------------
# Model Mimarisi (ESA)
# -------------------------
apply_attention_on: node
graph_dim: 256
edge_dim: 64
hidden_dims: [256, 256, 256, 256, 256, 256, 256, 256]
num_heads: [16, 16, 16, 16, 16, 16, 16, 16]
layer_types: ["M", "S", "M", "S", "M", "S", "P", "P"]
xformers_or_torch_attn: flash_varlen  # ⚡ Padding-free for mixed sizes (CORRECT: flash_varlen not varlen_flash!)
norm_type: LN
use_3d_coordinates: true
set_max_items: 0
triu_attn_mask: false

# -------------------------
# MLP Ayarları
# -------------------------
use_mlps: true
mlp_hidden_size: 512
num_mlp_layers: 3
mlp_type: standard
mlp_dropout: 0.0
use_mlp_ln: false
pre_or_post: pre

# -------------------------
# Dropout Ayarları
# -------------------------
sab_dropout: 0.0
mab_dropout: 0.0
pma_dropout: 0.0
attn_residual_dropout: 0.0
pma_residual_dropout: 0.0

# -------------------------
# 3D Geometrik Parametreler
# -------------------------
molecule_max_atoms: null
gaussian_kernels: 128
cutoff_distance: 5.0
max_neighbors: 64
max_distance: 20.0
distance_bins: 5

# -------------------------
# Ön-Eğitim Görevleri
# -------------------------
pretraining_tasks: ["long_range_distance", "short_range_distance", "mlm"]
task_weights:
  long_range_distance: 1.5
  short_range_distance: 1.0
  mlm: 0.3
mlm_mask_ratio: 0.15
temperature: 0.2

# -------------------------
# Yeni Kod Yapısı İçin Gerekli Diğer Parametreler
# -------------------------
max_block_types: 150
max_position_types: 15
max_entity_types: 6
block_embedding_dim: 32
position_embedding_dim: 16
entity_embedding_dim: 8
use_hierarchical_features: false
max_atomic_num: 118
rwse_dim: 24
lap_dim: 4
mlp_inter_dim: 128
pma_num_outputs: 32
max_node_items: 2000
max_edge_items: 4000
output_dim: 1
posenc: null
use_memory_efficient_attention: true
attention_dropout: 0.1
preserve_universal_blocks: true

# ===================================================================
# Quick Start Guide
# ===================================================================
#
# 1. PROTEIN-ONLY MODE (Default):
#    - Set multi_domain.enabled = false
#    - Run: python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
#
# 2. MULTI-DOMAIN MODE (When you have QM9 and Metabolite data):
#    a. Generate metadata for all chunks:
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/proteins/processed_graphs_40k
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/small_molecules/qm9/processed_chunks
#       python data_loading/create_chunk_metadata.py --chunk-dir ../data/metabolites/processed_chunks
#
#    b. Update this config:
#       - Set multi_domain.enabled = true
#       - Set multi_domain.datasets.qm9.enabled = true
#       - Set multi_domain.datasets.metabolite.enabled = true
#       - Adjust atom_ratio values if needed (will be auto-normalized)
#
#    c. Run training:
#       python core/train_pretrain.py --config-yaml-path core/pretraining_config_multidomain.yaml
#
# 3. TUNING PARAMETERS:
#    - target_atoms_per_rotation: Increase if you have more GPU RAM, decrease if OOM
#    - max_cache_chunks: Increase if you have more system RAM
#    - atom_ratio: Adjust to control dataset contribution (doesn't need to sum to 100)
#
# ===================================================================

